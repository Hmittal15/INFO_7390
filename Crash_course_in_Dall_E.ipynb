{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hmittal15/INFO_7390/blob/main/Crash_course_in_Dall_E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5f4553",
      "metadata": {
        "id": "ac5f4553"
      },
      "source": [
        "# DALL·E\n",
        "\n",
        "*Authors*   \n",
        "Harshit Mittal, Nik Bear Brown "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e0a9b2",
      "metadata": {
        "id": "55e0a9b2"
      },
      "source": [
        "### Table of contents:-\n",
        "\n",
        "- What is DALL·E?\n",
        "- Evolution of DALL·E\n",
        "- Incalculable applications\n",
        "- Superpowers of DALL·E\n",
        "- DALL·E V/S DALL·E 2\n",
        "- How does DALL·E 2 work?\n",
        "  - A 2-min read explanation\n",
        "  - Let's dive deep now!\n",
        "    - Step 1: Text encoding\n",
        "        - Sidenote: CLIP model training\n",
        "    - Step 2: Image encoding\n",
        "        - Sidenote:\n",
        "          - Diffusion models\n",
        "          - Transformer\n",
        "    - Step 3: Decoding\n",
        "- Implementing DALL·E 2 through python script\n",
        "- Use DALL·E 2 effectively\n",
        "- Limitations of DALL·E 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0932391d",
      "metadata": {
        "id": "0932391d"
      },
      "source": [
        "### What is DALL·E?\n",
        "\n",
        "![Dall-E_Detective_Illustration](https://user-images.githubusercontent.com/108916132/217738636-212a1661-ea1b-4889-9585-e1be5b2184ec.png)\n",
        "\n",
        "Dialing into 1980s, Detective Jones walks into the room with a file in hand. He approaches a sketch artist, Sarah, who is seated at her desk with a pencil and sketchpad. Detective Jones explaining the crime scene to the artist: the witness saw a man running away from the scene, he was a tall guy, around 6'2, with a muscular build and short, dark hair. Sarah carefully adds all the details to the sketch and produces it before the detective. Detective Jones examined the sketch and lauds, \"Perfect. This is exactly what we needed. Thanks, Sarah.\"\n",
        "\n",
        "What if I say that we all have such a sketch artist available with us 24\\*7 at our finger-tips? Yes, with advancements in technology, this fascination has come to life!\n",
        "\n",
        "DALL·E is a state-of-the-art artificial intelligence program developed by OpenAI that creates images from textual descriptions. Given a textual prompt, DALL·E generates an original image based on that description, showcasing its ability to understand and execute creative tasks. The model has been trained on a diverse dataset of images and can generate a wide range of images, from photorealistic to highly stylized illustrations. This technology has the potential to revolutionize fields such as advertising, product design, and visual storytelling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55385859",
      "metadata": {
        "id": "55385859"
      },
      "source": [
        "<img width=\"955\" alt=\"Dall-E_Robot_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221384925-ef576131-8b61-4d87-94e4-b1e44b1d61a2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1fc16b",
      "metadata": {
        "id": "ca1fc16b"
      },
      "source": [
        "<img width=\"956\" alt=\"Dall-E_Robot_2_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221384935-f0516537-f0b3-4ea9-80a2-0f089cd09ada.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25dec18",
      "metadata": {
        "id": "d25dec18"
      },
      "source": [
        "### Evolution of DALL·E\n",
        "\n",
        "* __January 2021__: OpenAI reveals DALL-E in a blog post. DALL-E is a model that uses a modified version of GPT-3 to generate images.\n",
        "\n",
        "\n",
        "* __April 2022__: OpenAI announces the development of DALL-E 2, a successor designed to generate more realistic images at higher resolutions and combine concepts, attributes, and styles. OpenAI does not release source code for either model.\n",
        "\n",
        "\n",
        "* __July 20, 2022__: DALL-E 2 enters into beta phase with invitations sent to 1 million waitlisted individuals. Access to DALL-E 2 was previously restricted to pre-selected users for a research preview due to concerns about ethics and safety.\n",
        "\n",
        "\n",
        "* __September 28, 2022__: DALL-E 2 is opened to anyone and the waitlist requirement is removed.\n",
        "\n",
        "\n",
        "* __Early November 2022__: OpenAI releases DALL-E 2 as an API, allowing developers to integrate the model into their own applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec0a3d0",
      "metadata": {
        "id": "dec0a3d0"
      },
      "source": [
        "### Incalculable applications\n",
        "\n",
        "DALL-E has a wide range of potential applications in various industries, some of which include:\n",
        "\n",
        "* __Art and Design__: DALL-E's ability to generate unique and imaginative images based on textual descriptions opens up new possibilities for artists and designers. For example, an artist could describe a concept they have in mind, and DALL-E could generate an image that captures the essence of that idea. This could be especially useful for digital artists who work with 3D modeling, animation, and other digital media.\n",
        "\n",
        "\n",
        "* __Advertising__: DALL-E can be used to generate images for advertisements, making it easier for marketers to create engaging and memorable campaigns. For example, a company could describe the product or service they want to advertise, and DALL-E could generate images that capture the essence of that offering. This could be useful for creating visual content for social media, online advertisements, or print materials.\n",
        "\n",
        "\n",
        "* __Product Visualization__: DALL-E can be used to generate images of products, allowing companies to quickly and easily showcase new offerings. For example, a company could describe a new product, and DALL-E could generate images that give consumers an idea of what the product looks like and how it functions. This could be especially useful for companies in the retail or e-commerce industries that need to showcase their products in a visually appealing way.\n",
        "\n",
        "\n",
        "* __Movie and Video Game Concept Art__: DALL-E can be used to generate concept art for movies and video games. For example, a movie director or video game designer could describe a scene or character they have in mind, and DALL-E could generate images that capture the essence of that idea. This could be useful for quickly generating initial concept art or for exploring different visual directions for a project.\n",
        "\n",
        "\n",
        "* __Web Design__: DALL-E could be used to generate unique and imaginative images for websites. For example, a web designer could describe the look and feel they want for a particular section of a website, and DALL-E could generate images that match that description. This could be useful for creating custom graphics or backgrounds for websites.\n",
        "\n",
        "\n",
        "These are just a few examples of the potential applications of DALL-E. The technology is still new, and it's likely that creative individuals and businesses will find new and innovative ways to use it in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15744f7",
      "metadata": {
        "id": "b15744f7"
      },
      "source": [
        "### Superpowers of DALL·E\n",
        "\n",
        "The model is trained on a large dataset of text-image pairs and can generate diverse images with different styles and contents.\n",
        "\n",
        "Here's a list of what superpowers DALL·E has and what can it do:\n",
        "\n",
        "* __Image synthesis__: One of the key capabilities of DALL·E 2 is its ability to generate new images from textual descriptions. For example, if we provide the model with a description such as \"a giraffe reading a book on a sunny day,\" it can generate a unique, high-resolution image of such a scene. The model is capable of synthesizing images of diverse objects, scenes, and animals, and can generate images with intricate details and textures.\n",
        "<img width=\"956\" alt=\"Dall-E_Giraffe_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/217738637-f58cd1bf-7c00-45d9-b88d-9cd70328a3a5.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ac5ac7",
      "metadata": {
        "id": "b1ac5ac7"
      },
      "source": [
        "* __Style transfer__: DALL·E 2 can also apply specific styles to existing images. For instance, it can turn a photograph into a painting or a comic book, or make a piece of architecture look like it was made of candy. The model can transfer the style of any reference image to a target image, resulting in a unique and creative output.\n",
        "<img width=\"955\" alt=\"Dall-E_Style_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/217738643-468a5649-c227-4ae9-a5ea-7040217cf2a3.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8864bce2",
      "metadata": {
        "id": "8864bce2"
      },
      "source": [
        "* __Image completion__: DALL·E 2 can also complete missing parts of an image based on the surrounding context. For example, if we provide the model with an image of a partially-obscured object, it can fill in the missing parts based on the visible parts and the surrounding context. This capability can be useful in a variety of applications, such as restoring damaged or corrupted images.\n",
        "\n",
        "Original image:\n",
        "![hospital](https://user-images.githubusercontent.com/108916132/217738646-40acec67-5526-465a-9c7e-839eae972e1b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc808c1",
      "metadata": {
        "id": "bdc808c1"
      },
      "source": [
        "Completed image:\n",
        "<img width=\"1104\" alt=\"Dall-E_ImageFrameAdd_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/217738638-01c093bc-5951-4da7-92fe-61ef31d7c502.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eceb73c",
      "metadata": {
        "id": "9eceb73c"
      },
      "source": [
        "### DALL·E V/S DALL·E 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "532a69df",
      "metadata": {
        "id": "532a69df"
      },
      "source": [
        "| DALL·E 1      | DALL·E 2 |\n",
        "| :---        |    ----:   |\n",
        "| It generates 256 × 256 pixel images from text <br>via Discreet Variational Auto-Encoder | It generates 1024 x 1024 pixel images from text <br>using diffusion models |\n",
        "| DALL·E 1 model architecture involves 12-billion parameter   | DALL·E 2 works on a 3.5-billion parameter model and another <br>1.5-billion parameter model to enhance the resolution of its images |\n",
        "| DALL-E could only render AI-created images in a cartoonish fashion | DALL-E 2 can produce realistic images |\n",
        "| No image variations | It can generate as many variations inspired by original image |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a22bc1f9",
      "metadata": {
        "id": "a22bc1f9"
      },
      "source": [
        "<img width=\"500\" alt=\"Dall-E_Comparison_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/217738634-c31d5507-1015-4abb-969b-6c65907bd66b.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38843db7",
      "metadata": {
        "id": "38843db7"
      },
      "source": [
        "### How does DALL·E 2 work?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b5ff71",
      "metadata": {
        "id": "25b5ff71"
      },
      "source": [
        "As previously mentioned, DALL-E 2 operates by accepting text inputs and generating corresponding images as outputs. However, this capability is not achieved through a single process, but rather through the integration of various techniques that have undergone gradual development and refinement over recent years. These techniques are layered upon each other, much like building blocks, to form the complete model. To grasp a comprehension of DALL-E 2, it is important to recognize the individual components and their functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf711c58",
      "metadata": {
        "id": "cf711c58"
      },
      "source": [
        "<img width=\"600\" alt=\"Dall-E_Architecture_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/217738632-4923e8af-d9c4-4e6b-9a08-002fe62c5f52.png\">\n",
        "\n",
        "#### A ___2-min read___ explanation\n",
        "At its core, DALL-E 2 operates in a straightforward manner:\n",
        "1. First, a text prompt is fed into a text encoder that has been trained to convert the prompt into a representative space.\n",
        "2. Then, a component referred to as the 'prior' takes the text encoding and maps it to a corresponding image encoding that reflects the semantic content of the text encoding.\n",
        "3. Finally, an image decoder generates a visual representation of this semantic information in the form of an image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b80768",
      "metadata": {
        "id": "d4b80768"
      },
      "source": [
        "#### Let's dive deep now!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3432cb",
      "metadata": {
        "id": "ca3432cb"
      },
      "source": [
        "Let's commence our examination by exploring the process through which DALL-E 2 associates and blends textual and visual abstractions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9470da43",
      "metadata": {
        "id": "9470da43"
      },
      "source": [
        "##### Step 1: Text encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0f1d3f",
      "metadata": {
        "id": "fe0f1d3f"
      },
      "source": [
        "The process begins by feeding a text encoder with the prompt which converts it into _'text embeddings'_. But what is a text embedding? An embedding can be described as an organized set of numbers that depict a text or image. A basic illustration of embeddings is seen in the ASCII representation of characters and numbers. However, unlike ASCII, embeddings are not predetermined but rather acquired through a neural network's learning process.\n",
        "\n",
        "The text embeddings used by DALL·E 2 are produced by a separate neural network known as _'CLIP'_, developed by OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905581d4",
      "metadata": {
        "id": "905581d4"
      },
      "source": [
        "<img width=\"200\" alt=\"Dall-E_TextEncoding_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221384962-1e422440-fd79-4eee-b0a6-c32f0f8478e1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc478e8",
      "metadata": {
        "id": "0bc478e8"
      },
      "source": [
        "___Sidenote: CLIP model training___\n",
        "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CLIP, which stands for 'Contrastive Language-Image Pre-training', is a neural network model that determines the most fitting caption for a given image. It has been trained on 400 million images and their accompanying captions, allowing it to comprehend the correlation between a specific text snippet and an image. Unlike other models that attempt to predict a caption given an image, CLIP focuses on understanding the relationship between a caption and an image. This contrastive approach to learning enables CLIP to identify the connection between the textual and visual depictions of the same object. In contrast to DALL·E 2's text-to-image generation, CLIP's objective is not to predict an image or classify it, but rather to learn the link between the textual and visual representations of the same object. The basic concept behind CLIP's training is straightforward.\n",
        "<br>\n",
        "1. Produce encodings of both image and text for each image-caption pair.\n",
        "2. Determine the cosine similarity between each image and text embedding pair.\n",
        "3. Continuously reduce the cosine similarity between mismatched image-caption pairs and increase the cosine similarity between corresponding image-caption pairs.\n",
        "\n",
        "<img width=\"519\" alt=\"Dall-E_CLIPTraining_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385094-d8a2e8db-882b-4985-bce5-c87c9ef89fe5.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06cc16eb",
      "metadata": {
        "id": "06cc16eb"
      },
      "source": [
        "Once the training is complete, the CLIP model is frozen, and CLIP text encoder is used to generate embeddings from the provided text prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d672d7fe",
      "metadata": {
        "id": "d672d7fe"
      },
      "source": [
        "##### Step 2: Image encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a6ea2c",
      "metadata": {
        "id": "05a6ea2c"
      },
      "source": [
        "The generation of text and image encoding intermediates in the DALL·E 2 model are referred to as CLIP embeddings, however it is not the CLIP encoder itself that produces the image embeddings. Instead, a separate model called the _'prior'_ is utilized to generate the _'image embeddings'_ based on the text embeddings generated by the CLIP text encoder. During the development of DALL·E 2, two options were considered for the prior: an Autoregressive model and a Diffusion model. Both options showed similar performance, but the Diffusion model was found to be more computationally efficient and was ultimately chosen as the preferred prior for DALL·E 2. The use of a prior enables DALL·E 2 to create variations of images.\n",
        "<br>The diffusion prior is a method that helps generate new image embeddings based on a given text description. This method uses a special type of machine learning model called a _'Transformer'_, which is trained on several different inputs.\n",
        "\n",
        "The inputs to the Transformer include:\n",
        "<br>(i) The encoded text: This is the encoded text description that is fed into the model.\n",
        "<br>(ii) The CLIP text embedding: This is a special representation of the text that is created by encoding the description using CLIP model.\n",
        "<br>(iii) An embedding for the diffusion timestep: This is a representation of the current step in the image generation process.\n",
        "<br>(iv) The noised CLIP image embedding: This is a representation of an image that has been deliberately altered to make the model more robust to overfitting.\n",
        "<br>(v) A final embedding: This is a representation of the output from the Transformer, which is used to predict the unnoised CLIP image embedding.\n",
        "\n",
        "In order to improve the quality of the generated images, the model does not use a simple dot product to condition the diffusion prior. Instead, it generates two samples of an image embedding and selects the one with a higher dot product with the text embedding. This helps ensure that the generated image is a better match for the text description."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b949b3",
      "metadata": {
        "id": "f4b949b3"
      },
      "source": [
        "___Sidenote:___\n",
        "<br>___Diffusion models___\n",
        "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Diffusion models utilize the transformer architecture to generate data. The process begins by taking a piece of information, such as an image, and introducing increasingly more randomness over a series of time steps until it becomes unrecognizable. Then, the model attempts to restore the image back to its original form. Through this process, the model learns to create new images or other forms of data.\n",
        "\n",
        "<img width=\"604\" alt=\"Dall-E_Diffusion_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385104-9c373595-441c-4107-89d3-07333b8ba005.png\">\n",
        "\n",
        "The process of adding noise, depicted in the illustration, is seen as a Markov chain that modifies an image through incremental amounts of randomness until it becomes indistinguishable from pure Gaussian noise. The Diffusion Model is trained to retrace the steps of this process by removing the noise over a series of time steps. After completing training, the Diffusion Model can be divided into two parts, one of which can be utilized to generate new images by randomly sampling Gaussian noise and then using the model to remove the noise and produce a realistic image.\n",
        "\n",
        "___Transformer___\n",
        "<br>A Transformer is a type of machine learning model used in natural language processing and computer vision tasks. In the case of the diffusion prior, the Transformer is being used to generate new image embeddings based on a given text description. A Transformer is called this because it uses a type of attention mechanism called _'self-attention\"_ to transform input data into a different representation. It does this by considering the relationships between different elements in the input data, such as words in a sentence or pixels in an image. The Transformer-based approach is used in the diffusion prior because it allows the model to consider the relationships between different elements in the input data. This is particularly important when generating an image based on a text description, as the model needs to understand the relationships between the words in the text and the pixels in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f2ef11",
      "metadata": {
        "id": "46f2ef11"
      },
      "source": [
        "<img width=\"338\" alt=\"Dall-E_Transformer_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385133-5a5525c0-3a91-4535-b973-387750e5d122.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f36cce",
      "metadata": {
        "id": "00f36cce"
      },
      "source": [
        "##### Step 3: Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc71781b",
      "metadata": {
        "id": "cc71781b"
      },
      "source": [
        "OpenAI has also developed a model known as _'GLIDE'_ (Guided Language to Image Diffusion for Generation and Editing) as a variation of the decoder. Unlike an autoencoder which solely aims to reconstruct an image given its embedding, GLIDE's objective is to generate an image that preserves the key features of the original image based on its embedding. GLIDE represents an advanced form of the Diffusion Model by incorporating textual information into the training process. Unlike the standard Diffusion Model, which starts from random Gaussian noise and lacks direction, GLIDE enhances the Diffusion Model's generative capabilities by integrating additional textual embeddings into the model's existing time step embedding. This leads to the capability of text-conditional image generation. DALL-E 2 further enhanced the GLIDE model by integrating four additional context tokens into the output sequence of the GLIDE text encoder, resulting in the ability to edit images through the use of text prompts.\n",
        "\n",
        "<img width=\"468\" alt=\"Dall-E_GLIDE_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385139-d08b6323-c460-4e47-ba9b-ba2a979e0fa1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b66c503",
      "metadata": {
        "id": "9b66c503"
      },
      "source": [
        "### Implementing DALL·E 2 through python script:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an implementation of image generation from DALL·E 2 through the OpenAI DALL·E 2 API. To use the DALL-E 2 API, we will need to sign up for an API key on the OpenAI website. Once we have an API key, we can make API calls to generate images. The API is a REST API, which means you can make requests to it using HTTP methods like GET, POST, and DELETE.\n",
        "<br>\n",
        "<br>\n",
        "NOTE: For the purpose of demonstration, I have stored my OpenAI account's secret API key in a Google drive file. Then I'm importing that JSON file and reading the secret key to establish connection."
      ],
      "metadata": {
        "id": "pe69JxwdkBUm"
      },
      "id": "pe69JxwdkBUm"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2ef76dbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ef76dbd",
        "outputId": "0d056970-fc47-4d71-b91c-7458a301be0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#The drive module from the google.colab library is imported to mount Google Drive for access to a JSON file with the OpenAI API key.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4598cb9a",
      "metadata": {
        "id": "4598cb9a"
      },
      "outputs": [],
      "source": [
        "#The json module is also imported to read the API key from the JSON file.\n",
        "\n",
        "import json\n",
        "with open('/content/gdrive/MyDrive/OpenAI_API_secret.json') as f:\n",
        "    secrets = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2135de67",
      "metadata": {
        "id": "2135de67"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "import urllib\n",
        "import urllib.request\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "pd.options.display.max_colwidth = 1000\n",
        "\n",
        "class Dalle2_picture_generation():\n",
        "    \n",
        "    #initializes some important parameters, such as the bearer token and batch sizes for inpainting and regular images.\n",
        "    def __init__(self, bearer):\n",
        "        self.bearer = bearer\n",
        "        self.batch_size = 4\n",
        "        self.inpainting_batch_size = 4\n",
        "        self.task_sleep_seconds = 2\n",
        "\n",
        "    #takes in a prompt as an argument and creates an image based on the prompt using the OpenAI API.\n",
        "    def create_image(self, prompt):\n",
        "        body = { \"task_type\": \"text2im\", \"prompt\": { \"caption\": prompt, \"batch_size\": self.batch_size } }\n",
        "        return self.action_proceedings(body)\n",
        "    \n",
        "    #takes in a prompt and a number of images to generate and returns a list of generated images.\n",
        "    def create_number(self, prompt, number_image):\n",
        "        if number_image < self.batch_size:\n",
        "            raise ValueError(\"Please provide more number of images to generate\")\n",
        "        return [self.create_image(prompt) for _ in range (math.ceil (number_image / self.batch_size))]\n",
        "\n",
        "    #takes in a prompt and an optional image directory and returns a downloaded image.\n",
        "    def create_with_download(self, prompt, image_dir=os.getcwd()):\n",
        "        my_image = self.create_image(prompt)\n",
        "        if not my_image:\n",
        "            return None\n",
        "        return self.download_image(my_image, image_dir)\n",
        "\n",
        "    #takes in a JSON body as an argument, which contains the prompt and other necessary information.\n",
        "    #returns the task ID. It then sends GET requests to the API to check the status of the task until the task is either completed, failed, or rejected.\n",
        "    def action_proceedings(self, body):\n",
        "        openAIUrl= \"https://labs.openai.com/api/labs/tasks\"\n",
        "        headers= { 'Authorization': \"Bearer \" + self.bearer, 'Content-Type': \"application/json\" }\n",
        "\n",
        "        response= requests.post (openAIUrl, headers=headers, data=json.dumps(body))\n",
        "        if response.status_code != 200:\n",
        "            print(response.text)\n",
        "            return None\n",
        "        data= response.json()\n",
        "        print(f\"Task created and respecive ID is: {data['id']}\")\n",
        "        print(\"Task in progress...\")\n",
        "\n",
        "        while True:\n",
        "            requested_url = f\"https://labs.openai.com/api/labs/tasks/{data['id']}\"\n",
        "            response = requests.get(requested_url, headers=headers)\n",
        "            data = response.json()\n",
        "\n",
        "            if data[\"status\"] == \"failed\":\n",
        "                print(f\"Requested task failed. Please find the details: {data['status_information']}\")\n",
        "                return None\n",
        "\n",
        "            if data[\"status\"] == \"rejected\":\n",
        "                print(f\"Requested task got rejected. Please find the details: {data['status_information']}\")\n",
        "                return None\n",
        "\n",
        "            if not response.ok:\n",
        "                print(f\"Request got failed. Please find the status: {response.status_code}, data: {response.json()}\")\n",
        "                return None\n",
        "            \n",
        "            if data[\"status\"] == \"succeeded\":\n",
        "                print(\"Task completed successfully!\")\n",
        "                return data[\"generations\"][\"data\"]\n",
        "\n",
        "            time.sleep(self.task_sleep_seconds)\n",
        "\n",
        "    #takes in a generated image and an optional directory path and downloads the image to the specified directory.\n",
        "    #It returns a dictionary containing the URL and path of the downloaded image.\n",
        "    def download_image(self, my_image, image_dir=os.getcwd()):\n",
        "        if not my_image:\n",
        "            raise ValueError(\"Couldn't create image as data is empty!\")\n",
        "\n",
        "        file_directory = []\n",
        "        url_directory = []\n",
        "        \n",
        "        for image in my_image:\n",
        "            url_image = image[\"generation\"][\"image_path\"]\n",
        "            file_path = Path(image_dir, image['id']).with_suffix('.webp')\n",
        "            file_directory.append(str(file_path))\n",
        "            url_directory.append(str(url_image))\n",
        "            urllib.request.urlretrieve (url_image, file_path)\n",
        "            print(f\"Image downloaded successfully: {file_path}\")\n",
        "\n",
        "        df= pd.DataFrame(columns=['Image URL','Downloaded image filepath'])\n",
        "        for i, fd in enumerate(file_directory):\n",
        "          df.loc[i]= [url_directory[i], file_directory[i]]\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate the desired imges, we'll pass on the text prompt as an argument in \"create_with_download()\" method. This would return web URLs for the generated images under \"Image URL\" column heading, and would also download the images into local directory."
      ],
      "metadata": {
        "id": "yW1otYj6nJXF"
      },
      "id": "yW1otYj6nJXF"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "01de81ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "01de81ba",
        "outputId": "bdd3e2b5-df1a-4491-a6e6-5a4f7d867053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task created and respecive ID is: task-oF25PRHGDKwzcLOn5YYqfUVM\n",
            "Task in progress...\n",
            "Task completed successfully!\n",
            "Image downloaded successfully: /content/generation-SPC1RvtwqA5RKmIyLSMjquG9.webp\n",
            "Image downloaded successfully: /content/generation-RuWV1034kvx6TR0KkU6Q1Jgd.webp\n",
            "Image downloaded successfully: /content/generation-sjqFVVe2u2W9ixxJFhgxFcOF.webp\n",
            "Image downloaded successfully: /content/generation-GYWi4SBO21Bxeck5Oet5hTAN.webp\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Image URL  \\\n",
              "0  https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-SPC1RvtwqA5RKmIyLSMjquG9/image.webp?st=2023-02-26T00%3A36%3A40Z&se=2023-02-26T02%3A34%3A40Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/webp&skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-26T01%3A33%3A08Z&ske=2023-03-05T01%3A33%3A08Z&sks=b&skv=2021-08-06&sig=D%2B7XmjG4QKPzTWdwbTbAUtx0Vg4nGqb1PDCrs4RjVf8%3D   \n",
              "1    https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-RuWV1034kvx6TR0KkU6Q1Jgd/image.webp?st=2023-02-26T00%3A36%3A40Z&se=2023-02-26T02%3A34%3A40Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/webp&skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-26T01%3A33%3A08Z&ske=2023-03-05T01%3A33%3A08Z&sks=b&skv=2021-08-06&sig=aOfhABjDUnLUFvoEhwU0rJfwpCjiaIzO1RD10Tt/THM%3D   \n",
              "2  https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-sjqFVVe2u2W9ixxJFhgxFcOF/image.webp?st=2023-02-26T00%3A36%3A40Z&se=2023-02-26T02%3A34%3A40Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/webp&skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-26T01%3A33%3A08Z&ske=2023-03-05T01%3A33%3A08Z&sks=b&skv=2021-08-06&sig=50MXOvg2GLNvnI/hef7Lp%2B2gG7MlP3c3VOE6LK7afaw%3D   \n",
              "3  https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-GYWi4SBO21Bxeck5Oet5hTAN/image.webp?st=2023-02-26T00%3A36%3A40Z&se=2023-02-26T02%3A34%3A40Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/webp&skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-02-26T01%3A33%3A08Z&ske=2023-03-05T01%3A33%3A08Z&sks=b&skv=2021-08-06&sig=xmztI6HGa%2B3vKn1wJmmX4keyRdNgLKkV9Y6uCkCN7Zw%3D   \n",
              "\n",
              "                           Downloaded image filepath  \n",
              "0  /content/generation-SPC1RvtwqA5RKmIyLSMjquG9.webp  \n",
              "1  /content/generation-RuWV1034kvx6TR0KkU6Q1Jgd.webp  \n",
              "2  /content/generation-sjqFVVe2u2W9ixxJFhgxFcOF.webp  \n",
              "3  /content/generation-GYWi4SBO21Bxeck5Oet5hTAN.webp  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07a8cda0-9967-4a61-8aaf-abc72f36f756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image URL</th>\n",
              "      <th>Downloaded image filepath</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-SPC1RvtwqA5RKmIyLSMjquG9/image.webp?st=2023-02-26T00%3A36%3A40Z&amp;se=2023-02-26T02%3A34%3A40Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/webp&amp;skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-02-26T01%3A33%3A08Z&amp;ske=2023-03-05T01%3A33%3A08Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=D%2B7XmjG4QKPzTWdwbTbAUtx0Vg4nGqb1PDCrs4RjVf8%3D</td>\n",
              "      <td>/content/generation-SPC1RvtwqA5RKmIyLSMjquG9.webp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-RuWV1034kvx6TR0KkU6Q1Jgd/image.webp?st=2023-02-26T00%3A36%3A40Z&amp;se=2023-02-26T02%3A34%3A40Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/webp&amp;skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-02-26T01%3A33%3A08Z&amp;ske=2023-03-05T01%3A33%3A08Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=aOfhABjDUnLUFvoEhwU0rJfwpCjiaIzO1RD10Tt/THM%3D</td>\n",
              "      <td>/content/generation-RuWV1034kvx6TR0KkU6Q1Jgd.webp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-sjqFVVe2u2W9ixxJFhgxFcOF/image.webp?st=2023-02-26T00%3A36%3A40Z&amp;se=2023-02-26T02%3A34%3A40Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/webp&amp;skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-02-26T01%3A33%3A08Z&amp;ske=2023-03-05T01%3A33%3A08Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=50MXOvg2GLNvnI/hef7Lp%2B2gG7MlP3c3VOE6LK7afaw%3D</td>\n",
              "      <td>/content/generation-sjqFVVe2u2W9ixxJFhgxFcOF.webp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://openailabsprodscus.blob.core.windows.net/private/user-C6HPBjjEuSDHAnBvdXYgjma5/generations/generation-GYWi4SBO21Bxeck5Oet5hTAN/image.webp?st=2023-02-26T00%3A36%3A40Z&amp;se=2023-02-26T02%3A34%3A40Z&amp;sp=r&amp;sv=2021-08-06&amp;sr=b&amp;rscd=inline&amp;rsct=image/webp&amp;skoid=15f0b47b-a152-4599-9e98-9cb4a58269f8&amp;sktid=a48cca56-e6da-484e-a814-9c849652bcb3&amp;skt=2023-02-26T01%3A33%3A08Z&amp;ske=2023-03-05T01%3A33%3A08Z&amp;sks=b&amp;skv=2021-08-06&amp;sig=xmztI6HGa%2B3vKn1wJmmX4keyRdNgLKkV9Y6uCkCN7Zw%3D</td>\n",
              "      <td>/content/generation-GYWi4SBO21Bxeck5Oet5hTAN.webp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07a8cda0-9967-4a61-8aaf-abc72f36f756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07a8cda0-9967-4a61-8aaf-abc72f36f756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07a8cda0-9967-4a61-8aaf-abc72f36f756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "dalle = Dalle2_picture_generation(secrets['Dalle_API_KEY'])\n",
        "file_path=dalle.create_with_download(\"a skunk robot drawing a picture of skunk robot\")\n",
        "file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d05ab72",
      "metadata": {
        "id": "7d05ab72"
      },
      "source": [
        "### Use DALL·E 2 effectively:\n",
        "\n",
        "Here are some tips on how to feed effective prompts to DALLE-2 to get good images:\n",
        "\n",
        "1. __Be clear and specific:__\n",
        "<br>DALLE2 works based on the prompts given to it, so it's important to be clear and specific about what we want the image to be. This includes details such as the *style, subject, and colors* we want in the image.\n",
        "\n",
        "Before: \"Generate a picture of a car\"\n",
        "<img width=\"953\" alt=\"Dall-E_Car_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385142-f751a475-f9fc-4f04-be86-24939392bdff.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbe435c",
      "metadata": {
        "id": "1cbe435c"
      },
      "source": [
        "After: \"Please generate an image of a red sports car, with a glossy finish and racing stripes\"\n",
        "<img width=\"952\" alt=\"Dall-E_Car_2_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385144-430ab39d-6d6d-4f48-9f2f-c5a17afd17c9.png\">\n",
        "\n",
        "By adding more details to the prompt such as the color, finish, and design of the car, DALLE2 can create a more specific image that matches the requester's expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1249a062",
      "metadata": {
        "id": "1249a062"
      },
      "source": [
        "2. __Use simple language:__\n",
        "<br>Use simple and straightforward language when writing prompts for DALLE2. Avoid complex or convoluted phrasing, which could confuse the AI and result in suboptimal images.\n",
        "\n",
        "Before: \"Please generate a picture of a big red balloon floating in the sky\"\n",
        "<img width=\"953\" alt=\"Dall-E_Balloon_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385154-104eeaf9-f91b-4f20-b39e-df57ae19d58a.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03e30fe",
      "metadata": {
        "id": "a03e30fe"
      },
      "source": [
        "After: \"Please generate an image of a large red balloon in the sky, floating gracefully above the clouds\"\n",
        "<img width=\"953\" alt=\"Dall-E_Balloon_2_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385159-65314b17-936d-4880-aa67-f08fc4c1ef35.png\">\n",
        "\n",
        "Simplifying the language and using straightforward instructions can help DALLE2 better understand the prompt and generate more accurate images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf602d9f",
      "metadata": {
        "id": "cf602d9f"
      },
      "source": [
        "3. __Provide context:__\n",
        "<br>Provide context for the image we want DALLE2 to create. This could include information on the intended use of the image, the target audience, and any specific requirements or constraints.\n",
        "\n",
        "Before: \"Generate a picture of a cake\"\n",
        "<img width=\"953\" alt=\"Dall-E_Cake_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385167-d200afc4-5881-4c07-bbdf-20c3b451bb5d.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf58fec6",
      "metadata": {
        "id": "bf58fec6"
      },
      "source": [
        "After: \"Please generate an image of a three-tiered wedding cake, with white frosting and pink flowers, suitable for a traditional wedding reception\"\n",
        "<img width=\"950\" alt=\"Dall-E_Cake_2_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385174-7b379f6e-e6aa-4ef3-9383-ac1fd6f0b3a3.png\">\n",
        "\n",
        "Providing context for the image, such as its intended use for a wedding reception, can help DALLE2 better understand the requirements and generate an image that meets the requester's needs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8aea732",
      "metadata": {
        "id": "a8aea732"
      },
      "source": [
        "4. __Use examples:__\n",
        "<br>Providing examples of images that we like or want to emulate can help DALLE2 understand what we're looking for. We could include links to images or describe them in detail.\n",
        "\n",
        "Before: \"Generate a picture of a house\"\n",
        "<img width=\"950\" alt=\"Dall-E_Home_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385179-2cab772f-14ff-43ed-b539-f2e0733a9b8e.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f66501d",
      "metadata": {
        "id": "6f66501d"
      },
      "source": [
        "After: \"Please generate an image of a large two-story house, with a wrap-around porch and white picket fence, similar to this picture [link to a reference image]\"\n",
        "<img width=\"949\" alt=\"Dall-E_Home_2_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385183-8d19c785-cf82-4d14-8a7f-6437a9ec20c7.png\">\n",
        "\n",
        "Providing examples or reference images can help DALLE2 better understand the requester's preferences and generate an image that matches their expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e15083",
      "metadata": {
        "id": "33e15083"
      },
      "source": [
        "5. __Iterate and refine:__\n",
        "<br>DALLE2 may not always produce the exact image we're looking for on the first try. Try giving different variations of the same prompt and providing feedback to DALLE2 to help it improve its output.\n",
        "\n",
        "Before: \"Generate a picture of a tree\"\n",
        "<img width=\"949\" alt=\"Dall-E_Tree_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385191-33f9adf0-193d-43e4-b600-943db016d6a6.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21326dbc",
      "metadata": {
        "id": "21326dbc"
      },
      "source": [
        "After: \"Please generate an image of an oak tree with sprawling branches and golden leaves, as shown in the attached reference image. Could you make the leaves more vibrant and the trunk thicker?\"\n",
        "<img width=\"950\" alt=\"Dall-E_Tree_2_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221385196-dfc8bac4-bba7-4d68-9041-345922f1e4f0.png\">\n",
        "\n",
        "Providing feedback and making minor adjustments to the prompt can help DALLE2 generate images that are more closely aligned with the requester's needs and preferences.\n",
        "\n",
        "DALLE2 is an AI model and can only work with the information given to it in the prompt. By providing clear and specific instructions, we can help it produce high-quality images that meet our needs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations of DALL·E 2\n",
        "\n",
        "DALL-E 2 is an advanced AI model that is capable of generating highly realistic and detailed images based on text descriptions. However, like any machine learning model, it has certain limitations, particularly in generating certain types of images.\n",
        "\n",
        "1. __Scenes with multiple objects:__ DALL-E 2 may struggle with generating complex scenes that involve multiple objects and interactions between them. This is because it may not have sufficient training data to learn the complex relationships between different objects in a scene. Even in cases where the two characters are pop culture references that the model \"knows\" separately – for example, Batman and Superman – it can't seem to help blending them together. This means DALL-E 2 is often unable to meaningfully merge multiple objects or object properties such as shape, orientation, and color.\n",
        "\n",
        "<img width=\"955\" alt=\"Dall-E_Multiple_Objects_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221388447-5f03c629-c55a-4126-9535-2ce48885f072.png\">"
      ],
      "metadata": {
        "id": "Dfbb-q9jdytq"
      },
      "id": "Dfbb-q9jdytq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. __Dataset bias__:\n",
        "DALL-E 2 has biases and tendencies to depict people and environments as White/Western and engage in gender stereotypes, which is called representational bias. This occurs when the model reinforces stereotypes seen in the dataset that categorize people based on their identity. Specificity in prompts could reduce this problem, but the biases are inevitable since the internet has been predominantly white and Western, and datasets extracted from there will fall under the same biases. It also engages in gender stereotypes which can be seen in below illustration. When asked to draw a picture of a nurse, 3 out of the 4 returned images depicted a woman.\n",
        "\n",
        "<img width=\"955\" alt=\"Dall-E_Dataset_Bias_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221388716-86b2c247-38a8-4b14-a9bd-f4892eb86dc9.png\">"
      ],
      "metadata": {
        "id": "8ePxqQ5Cg2Db"
      },
      "id": "8ePxqQ5Cg2Db"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. __Spelling__:\n",
        "DALL-E 2 is great at drawing but horrible at spelling words because it doesn't encode spelling information from the text present in dataset images. When prompted with a phrase, it outputs approximations of the correct phrase, which isn't sufficient for spelling words. However, if DALL-E 2 were trained to encode the words in the images, it could be better at this task.\n",
        "\n",
        "<img width=\"950\" alt=\"Dall-E_Spelling_Illustration\" src=\"https://user-images.githubusercontent.com/108916132/221388869-510bb5f7-0d74-4a12-9dba-ed46e7e61b6d.png\">"
      ],
      "metadata": {
        "id": "oo9Em9u0iwzm"
      },
      "id": "oo9Em9u0iwzm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## License  \n",
        "\n",
        "All code in this notebook is available as open source through the MIT license.\n",
        "\n",
        "All text and images are free to use under the Creative Commons Attribution 3.0 license.  https://creativecommons.org/licenses/by/3.0/us/\n",
        "\n",
        "These licenses let people distribute, remix, tweak, and build upon the work, even commercially, as long as they give credit for the original creation.\n",
        "\n",
        "Copyright 2023 AI Skunks  https://github.com/aiskunks\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "8D7MHeQzZiF3"
      },
      "id": "8D7MHeQzZiF3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}